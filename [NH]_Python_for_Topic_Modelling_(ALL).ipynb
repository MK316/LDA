{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsnam95/my/blob/main/%5BNH%5D_Python_for_Topic_Modelling_(ALL).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import gensim\n",
        "from gensim import models\n",
        "from gensim.models import Phrases, CoherenceModel, LdaModel, LsiModel, HdpModel\n",
        "from gensim.corpora import Dictionary"
      ],
      "metadata": {
        "id": "T2lVWFpepB0o"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load data"
      ],
      "metadata": {
        "id": "MCNgzaQpgVQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "a = files.upload()\n",
        "fn = list(a.keys())[0]\n",
        "import pandas as pd\n",
        "df_news = pd.read_csv(fn)"
      ],
      "metadata": {
        "id": "7ISD7OhipXVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "clean-up"
      ],
      "metadata": {
        "id": "-mb3bXQGqk21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "black_list = [\n",
        "                                  \"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\",\"z\", # 개별 알파벳(alfabeto) \n",
        "                                       \n",
        "                                 \"al\", \"del\",\"de\",\"en\", \"hacia\", \"por\", \"para\", \"entre\",'sobre','según', #전치사(preposiciones)\n",
        "                                 \"con\",\"comigo\",\"contigo\", \"consigo\", \"sin\",\n",
        "\n",
        "                                 'cual', 'cuál', 'cuales', 'cuáles', 'cualquier', 'cualquiera', 'cualquieras', #관계사(relativos) & 의문사(interrogativos) (2)\n",
        "                                 'cuan', 'cuán', 'cuando', 'cuándo', 'cuanta','cuánta', 'cuantas', 'cuántas', 'cuanto', 'cuánto', 'cuantos', 'cuántos',   #관계사(relativos) & 의문사(interrogativos) (1)\n",
        "                                 'donde', 'dónde', 'adonde', 'adónde','como', 'cómo',                                  #관계사(relativos) & 의문사(interrogativos) (3)\n",
        "                                 'que', 'qué', 'quien', 'quién', 'quienes', 'quiénes','quienesquiera', 'quienquiera', #관계사(relativos) & 의문사(interrogativos) (4)\n",
        "                                 'cuyo', 'cuya','cuyos','cuyas', #관계사(relativos) & 의문사(interrogativos) (4)   \n",
        "\n",
        "                                 'el', 'la', 'las', 'le', 'les', 'lo', 'los', 'un', 'una', 'unas', 'uno', 'unos', #관사(articulos)\n",
        "                                 'yo', 'me', 'mi', 'mí', 'mia', 'mía', 'mias', 'mías','mio','mío', 'mios', 'míos', 'mis', #대명사(prombres)-1S\n",
        "                                 'nos', 'nosotras', 'nosotros', 'nuestra', 'nuestras', 'nuestro', 'nuestros', #대명사(prombres)-1P\n",
        "                                 'vosotras', 'vosotros', 'vuestra', 'vuestras', 'vuestro', 'vuestros', 'os', #대명사(prombres)-2P\n",
        "                                 'él', 'ella', 'ellas', 'ello', 'ellos', #대명사(prombres)-3S (1)\n",
        "                                 'ud', 'uds', 'usted','ustedes', 'vd', 'vds', 'se', 'su', 'sus', 'suya', 'suyas', 'suyo', 'suyos', #대명사(prombres)-3S (2)\n",
        "                                 'te', 'ti', 'tí', 'tu', 'tú', 'tus', 'tuya', 'tuyas', 'tuyo', 'tuyos', #대명사(prombres)-2S\n",
        "\n",
        "                                 #'este', 'esta', 'estos', 'estas', 'ese', 'esa', 'esos', 'esas','aquel', 'aquella', 'aquellos', 'aquellas', #지시사(demostrativos)\n",
        "\n",
        "                                 'era', 'erais', 'eramos', 'éramos', 'eran', 'eras',  #SER 동사활용형(1)\n",
        "                                 'eres', 'es', 'sea', 'seáis', 'seamos', 'sean', 'seas','sois', 'somos','son', 'soy', #SER 동사활용형(2)\n",
        "                                 'fue', 'fué', 'fui', 'fuí', 'fuimos', 'fuiste', 'fuisteis', #SER 동사활용형(3)\n",
        "                                 'fuera', 'fuerais', 'fuéramos', 'fueran', 'fueras', 'fueron', 'fuese', 'fueseis', 'fuésemos','fuesen', 'fueses', #SER 동사활용형(4)\n",
        "                                 'será', 'serán', 'serás', 'seré', 'seréis', 'seremos', #SER 동사활용형(5)\n",
        "                                 'sería', 'seríais', 'seríamos', 'serían', 'serías', #SER 동사활용형(6)\n",
        "                                 'sido', 'siendo', #SER 동사활용형(7)\n",
        "\n",
        "                                 'estar', 'está', 'estais', 'estáis', 'estamos', 'estan', 'están','estás', 'estoy',  #ESTAR 동사활용형(1)\n",
        "                                 'estaba', 'estabais', 'estábamos', 'estaban', 'estabas', #ESTAR 동사활용형(2)\n",
        "                                 'estada', 'estadas', 'estando', 'estad', #ESTAR 동사활용형(3)\n",
        "                                 'estará', 'estarán', 'estarás', 'estaré', 'estaréis', 'estaremos', #ESTAR 동사활용형(4)\n",
        "                                 'estaría', 'estaríais', 'estaríamos', 'estarían', 'estarías', #ESTAR 동사활용형(5)\n",
        "                                 'esté', 'estéis', 'estemos', 'estén', 'estés', #ESTAR 동사활용형(6) \n",
        "                                 'estuve', 'estuviera', 'estuvierais', 'estuviéramos', 'estuvieran', 'estuvieras', 'estuvieron', #ESTAR 동사활용형(7)\n",
        "                                 'estuviese', 'estuvieseis', 'estuviésemos', 'estuviesen', 'estuvieses', 'estuvimos', 'estuviste', 'estuvisteis', 'estuvo',  #ESTAR 동사활용형(8)\n",
        "\n",
        "                                 'haber', 'ha', 'habéis', 'han', 'has', 'hay', 'he', 'hemos',  #HABER 동사활용형(1)\n",
        "                                 'habia', 'había', 'habíais', 'habíamos', 'habían', 'habías',  #HABER 동사활용형(2)\n",
        "                                 'habida', 'habidas', 'habido', 'habidos', 'habiendo',         #HABER 동사활용형(3)\n",
        "                                 'habrá', 'habrán', 'habrás', 'habré', 'habréis', 'habremos',  #HABER 동사활용형(4)\n",
        "                                 'habría', 'habríais', 'habríamos', 'habrían', 'habrías',      #HABER 동사활용형(5)\n",
        "                                 'haya', 'hayáis', 'hayamos', 'hayan', 'hayas',                #HABER 동사활용형(6)\n",
        "                                 'hube', 'hubimos', 'hubiste', 'hubisteis', 'hubo',            #HABER 동사활용형(7)\n",
        "                                 'hubiera', 'hubierais', 'hubiéramos', 'hubieran', 'hubieras', 'hubieron',  #HABER 동사활용형(8) \n",
        "                                 'hubiese', 'hubieseis', 'hubiésemos', 'hubiesen', 'hubieses', #HABER 동사활용형(9) \n",
        "\n",
        "                                 'ni','no','jamás','nunca','nada','ningún','ninguna','ninguno', #부정어(negativos)\n",
        "                                 'pero','mas', 'aunque','pues','porque','si','sí', #접속사(conjunciones)\n",
        "                                 'algo','algún','alguna','algunas','alguno','algunos', #부정어(indefinidos)\n",
        "                                 'embargo',                                     #기타(etc)                                                                   ))         \n",
        "  \n",
        "                                   \"LBR\",\n",
        "                                   \"ttl\",\n",
        "                                   \"mtl\",\n",
        "                                   \"ftl\",\n",
        "                                   \"=\",\"===============================================\",\n",
        "                                   \"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\"]"
      ],
      "metadata": {
        "id": "fQXEpuoHqKMR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop = set(stopwords.words('spanish'))\n",
        "additional_stopwords=set(black_list)\n",
        "stopwords = stop.union(additional_stopwords)"
      ],
      "metadata": {
        "id": "HvdtnQJeq98_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "!python -m spacy download es_core_news_sm\n",
        "nlp = spacy.load(\"es_core_news_sm\")"
      ],
      "metadata": {
        "id": "HTiSGf6_rEDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatization(texts):\n",
        "    texts_out = [token.text for token in nlp(texts) if token.text not in black_list and len(token.text)>2]\n",
        "    return texts_out"
      ],
      "metadata": {
        "id": "2d9a8dBznPUd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect phrases, based on collected collocation counts. Adjacent words that appear together more frequently than expected are joined together with the _ character.\n",
        "bigram = gensim.models.Phrases(df_news.texto.to_list())"
      ],
      "metadata": {
        "id": "HZXpUnfi6dOq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaner(word):\n",
        "  word = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*', '', word, flags=re.MULTILINE)\n",
        "  word = re.sub(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', \"\", word)\n",
        "  word = re.sub(r'ee.uu', 'eeuu', word)\n",
        "  word = re.sub(r'\\#\\.', '', word)\n",
        "  word = re.sub(r'\\n', '', word)\n",
        "  word = re.sub(r',', '', word)\n",
        "  word = re.sub(r'\\-', ' ', word)\n",
        "  word = re.sub(r'\\.{3}', ' ', word)\n",
        "  word = re.sub(r'a{2,}', 'a', word)\n",
        "  word = re.sub(r'é{2,}', 'é', word)\n",
        "  word = re.sub(r'i{2,}', 'i', word)\n",
        "  word = re.sub(r'ja{2,}', 'ja', word) \n",
        "  word = re.sub(r'á', 'a', word)\n",
        "  word = re.sub(r'é', 'e', word)\n",
        "  word = re.sub(r'í', 'i', word)\n",
        "  word = re.sub(r'ó', 'o', word)\n",
        "  word = re.sub(r'ú', 'u', word)  \n",
        "  word = re.sub('[^a-zA-Z]', ' ', word)\n",
        "  list_word_clean = []\n",
        "  for w1 in word.split(\" \"):\n",
        "    if  w1.lower() not in stopwords:\n",
        "      list_word_clean.append(w1.lower())\n",
        "\n",
        "  bigram_list = bigram[list_word_clean]\n",
        "  # 재조합해서 lemmatize한 list\n",
        "  out_text = lemmatization(\" \".join(bigram_list))\n",
        "  return out_text"
      ],
      "metadata": {
        "id": "n_3wAGvHnTSW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_news['texto'] = df_news['texto'].apply(cleaner)"
      ],
      "metadata": {
        "id": "WzlZ8FlLrcDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "prepare corpus for gensim topic modeling"
      ],
      "metadata": {
        "id": "cIosAjhsrdd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = Dictionary(df_news['texto'].to_list())\n",
        "dictionary.compactify() #??\n",
        "# Filter extremes\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.97, keep_n=None)\n",
        "dictionary.compactify() #??\n",
        "\n",
        "corpus = [dictionary.doc2bow(text) for text in df_news['texto'].to_list()]"
      ],
      "metadata": {
        "id": "MoqHmNhHre5-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate models"
      ],
      "metadata": {
        "id": "7Snf31_FQ6Uf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "preparing useful functions"
      ],
      "metadata": {
        "id": "4aM9U4t_kDed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "a function to display topics better (for HDP, LSI, LDA)"
      ],
      "metadata": {
        "id": "nGj3sl2Xjd5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_topics(model, model_type=\"lda\"): # for all models\n",
        "  for topic_idx, topic in enumerate(model.print_topics()):\n",
        "    print (\"Topic %d:\" % (topic_idx))\n",
        "    if model_type== \"hdp\":\n",
        "      print (\" \".join(re.findall( r'\\*(.[^\\*-S]+).?', topic[1])), \"\\n\")\n",
        "    else:\n",
        "      print (\" \".join(re.findall( r'\\\"(.[^\"]+).?', topic[1])), \"\\n\")"
      ],
      "metadata": {
        "id": "mysUf0HEr6-r"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "a function to evaluate the number of topics (for LSI, LDA only)"
      ],
      "metadata": {
        "id": "F1xZnAeARNwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_graph(dictionary, corpus, texts, limit, model): # for LSI, LDA\n",
        "    \"\"\"\n",
        "    Function to display num_topics - LDA graph using c_v coherence\n",
        "    \n",
        "    Parameters:\n",
        "    ----------\n",
        "    dictionary : Gensim dictionary\n",
        "    corpus : Gensim corpus\n",
        "    limit : topic limit\n",
        "    \n",
        "    Returns:\n",
        "    -------\n",
        "    lm_list : List of LDA topic models\n",
        "    c_v : Coherence values corresponding to the LDA model with respective number of topics\n",
        "    \"\"\"\n",
        "    c_v = []\n",
        "    lm_list = []\n",
        "    for num_topics in range(1, limit):\n",
        "        if model == 'lsi':\n",
        "          lm = LsiModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
        "        else:\n",
        "          lm = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
        "        lm_list.append(lm)\n",
        "        cm = CoherenceModel(model=lm, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        c_v.append(cm.get_coherence())\n",
        "        \n",
        "    # Show graph\n",
        "    x = range(1, limit)\n",
        "    plt.plot(x, c_v)\n",
        "    plt.xlabel(\"num_topics\")\n",
        "    plt.ylabel(\"Coherence score\")\n",
        "    plt.legend((\"c_v\"), loc='best')\n",
        "    plt.show()\n",
        "    \n",
        "    return lm_list, c_v"
      ],
      "metadata": {
        "id": "bJZNVNLpr77U"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Hierarchical Dirichlet process Model"
      ],
      "metadata": {
        "id": "v_lMuA4XrnXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "time.clock = time.time\n",
        "hdpmodel = HdpModel(corpus=corpus, id2word=dictionary, random_state= 30)"
      ],
      "metadata": {
        "id": "SHG7hYxCr3YD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Latent Semantic Indexing model"
      ],
      "metadata": {
        "id": "dSdNRkzwruDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lmlist_lsi, c_v = evaluate_graph(dictionary=dictionary, corpus=corpus, texts=df_news['texto'].to_list(), limit=21, model= \"lsi\")"
      ],
      "metadata": {
        "id": "4ejOEzZdsfvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Latent Dirichlet Allocation Model"
      ],
      "metadata": {
        "id": "xCQopweArvSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lmlist_lda, c_v = evaluate_graph(dictionary=dictionary, corpus=corpus, texts=df_news['texto'].to_list(), limit=21, model= \"lda\")"
      ],
      "metadata": {
        "id": "nu3IgpL6sqjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# select a best model for LSI and LDA and report the summary of all three models (HDP, LSI, LDA)"
      ],
      "metadata": {
        "id": "Ik-yrJhTsu6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ldamodel = lmlist_lda[5]\n",
        "lsimodel = lmlist_lsi[2]\n",
        "\n",
        "hdptopics = [[word for word, prob in topic] for topicid, topic in hdpmodel.show_topics(formatted=False)]\n",
        "lsitopics = [[word for word, prob in topic] for topicid, topic in lsimodel.show_topics(formatted=False)]\n",
        "ldatopics = [[word for word, prob in topic] for topicid, topic in ldamodel.show_topics(formatted=False)]"
      ],
      "metadata": {
        "id": "H2VUKYGosy72"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hdp_coherence = CoherenceModel(topics=hdptopics[:10], texts=df_news['texto'].to_list(), dictionary=dictionary, window_size=10).get_coherence() \n",
        "lsi_coherence = CoherenceModel(topics=lsitopics[:10], texts=df_news['texto'].to_list(), dictionary=dictionary, window_size=10).get_coherence() \n",
        "lda_coherence = CoherenceModel(topics=ldatopics, texts=df_news['texto'].to_list(), dictionary=dictionary, window_size=10).get_coherence() "
      ],
      "metadata": {
        "id": "YyicIxDts0LC"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "coherences = [hdp_coherence, lsi_coherence, lda_coherence]\n",
        "n = len(coherences)\n",
        "x = ['hdp_coherence', 'lsi_coherence', 'lda_coherence']\n",
        "sns.barplot(x, coherences)"
      ],
      "metadata": {
        "id": "ioSpCKlKs2E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assign a dominant topic to each document (with LDA selected)"
      ],
      "metadata": {
        "id": "AmjmbduBaVqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_topics_sentences(ldamodel=0, corpus=corpus, texts=0):\n",
        "    # Init output\n",
        "    sent_topics_df = pd.DataFrame()-n\n",
        "\n",
        "    # Get main topic in each document\n",
        "    for i, row in enumerate(ldamodel[corpus]):\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # => dominant topic\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "\n",
        "    # Add original text to the end of the output\n",
        "    contents = pd.Series(texts)\n",
        "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
        "    return(sent_topics_df)"
      ],
      "metadata": {
        "id": "1GIyFKV4te4z"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_topic_sents_keywords = format_topics_sentences(ldamodel, corpus=corpus, texts=df_news['texto'].to_list())\n",
        "\n",
        "# Format\n",
        "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
        "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
        "\n",
        "# Show\n",
        "df_dominant_topic.head(500)"
      ],
      "metadata": {
        "id": "dtTKmIW1tgG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# assign a specific name to a topic and possible analyises"
      ],
      "metadata": {
        "id": "-UlgNzJytkn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_dicc = {0:'TEMA1', 1:'TEMA2', 2:'TEMA3', 3: 'TEMA4', 4:'TEMA5', 5:'TEMA6', 6:'TEMA7', \n",
        "              7:'TEMA8', 8:'TEMA9', 9: 'TEMA10', 10:'TEMA11', 11:'TEMA12'}\n",
        "df_dominant_topic['Dominant_Topic'] = df_dominant_topic['Dominant_Topic'].astype('int64')\n",
        "df_dominant_topic['Dominant_Topic'] = df_dominant_topic['Dominant_Topic'].map(label_dicc)\n",
        "df_news['labels'] = df_dominant_topic['Dominant_Topic']"
      ],
      "metadata": {
        "id": "66DsHVBOtiQT"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's examine some text and its topics."
      ],
      "metadata": {
        "id": "Mufi6S8ltsNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_news[ df_news['labels'] == 'TEMA5'].head().texto"
      ],
      "metadata": {
        "id": "rVm-DEq9tv0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the distribution of topics to check if the topis are balanced\n"
      ],
      "metadata": {
        "id": "05B66e98txav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax = df_dominant_topic['Dominant_Topic'].value_counts().plot(kind='bar')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iLLKhG7wt39F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final visualization!"
      ],
      "metadata": {
        "id": "hAeUQrHXt8ce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "id": "LAYmBzj5uzNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis, pyLDAvis.gensim\n",
        "pyLDAvis.enable_notebook()\n",
        "pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)"
      ],
      "metadata": {
        "id": "G4bc0HDGu1IC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}