{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsnam95/my/blob/main/%5BNH%5D_Python_for_Topic_Modelling_(ALL).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "import gensim\n",
        "from gensim import models\n",
        "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
        "from gensim.corpora import Dictionary"
      ],
      "metadata": {
        "id": "T2lVWFpepB0o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce0aeafd-09b7-4497-a35d-ffbebb2423f1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "a = files.upload()"
      ],
      "metadata": {
        "id": "7ISD7OhipXVv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "dee8ab9f-46c8-4791-d3d0-8de27073625a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ad9438ca-cd13-40a4-95fb-12777e076175\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ad9438ca-cd13-40a4-95fb-12777e076175\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving a-machado-(CLEAN)-Revisado3-(solo libros)-nuevo.csv to a-machado-(CLEAN)-Revisado3-(solo libros)-nuevo (2).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_news = pd.read_csv(\"a-machado-(CLEAN)-Revisado3-(solo libros)-nuevo.csv\")"
      ],
      "metadata": {
        "id": "R_D20tWdpo8H"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### clean up"
      ],
      "metadata": {
        "id": "-mb3bXQGqk21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "black_list = [\n",
        "                                  \"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\",\"z\", # 개별 알파벳(alfabeto) \n",
        "                                       \n",
        "                                 \"al\", \"del\",\"de\",\"en\", \"hacia\", \"por\", \"para\", \"entre\",'sobre','según', #전치사(preposiciones)\n",
        "                                 \"con\",\"comigo\",\"contigo\", \"consigo\", \"sin\",\n",
        "\n",
        "                                 'cual', 'cuál', 'cuales', 'cuáles', 'cualquier', 'cualquiera', 'cualquieras', #관계사(relativos) & 의문사(interrogativos) (2)\n",
        "                                 'cuan', 'cuán', 'cuando', 'cuándo', 'cuanta','cuánta', 'cuantas', 'cuántas', 'cuanto', 'cuánto', 'cuantos', 'cuántos',   #관계사(relativos) & 의문사(interrogativos) (1)\n",
        "                                 'donde', 'dónde', 'adonde', 'adónde','como', 'cómo',                                  #관계사(relativos) & 의문사(interrogativos) (3)\n",
        "                                 'que', 'qué', 'quien', 'quién', 'quienes', 'quiénes','quienesquiera', 'quienquiera', #관계사(relativos) & 의문사(interrogativos) (4)\n",
        "                                 'cuyo', 'cuya','cuyos','cuyas', #관계사(relativos) & 의문사(interrogativos) (4)   \n",
        "\n",
        "                                 'el', 'la', 'las', 'le', 'les', 'lo', 'los', 'un', 'una', 'unas', 'uno', 'unos', #관사(articulos)\n",
        "                                 'yo', 'me', 'mi', 'mí', 'mia', 'mía', 'mias', 'mías','mio','mío', 'mios', 'míos', 'mis', #대명사(prombres)-1S\n",
        "                                 'nos', 'nosotras', 'nosotros', 'nuestra', 'nuestras', 'nuestro', 'nuestros', #대명사(prombres)-1P\n",
        "                                 'vosotras', 'vosotros', 'vuestra', 'vuestras', 'vuestro', 'vuestros', 'os', #대명사(prombres)-2P\n",
        "                                 'él', 'ella', 'ellas', 'ello', 'ellos', #대명사(prombres)-3S (1)\n",
        "                                 'ud', 'uds', 'usted','ustedes', 'vd', 'vds', 'se', 'su', 'sus', 'suya', 'suyas', 'suyo', 'suyos', #대명사(prombres)-3S (2)\n",
        "                                 'te', 'ti', 'tí', 'tu', 'tú', 'tus', 'tuya', 'tuyas', 'tuyo', 'tuyos', #대명사(prombres)-2S\n",
        "\n",
        "                                 #'este', 'esta', 'estos', 'estas', 'ese', 'esa', 'esos', 'esas','aquel', 'aquella', 'aquellos', 'aquellas', #지시사(demostrativos)\n",
        "\n",
        "                                 'era', 'erais', 'eramos', 'éramos', 'eran', 'eras',  #SER 동사활용형(1)\n",
        "                                 'eres', 'es', 'sea', 'seáis', 'seamos', 'sean', 'seas','sois', 'somos','son', 'soy', #SER 동사활용형(2)\n",
        "                                 'fue', 'fué', 'fui', 'fuí', 'fuimos', 'fuiste', 'fuisteis', #SER 동사활용형(3)\n",
        "                                 'fuera', 'fuerais', 'fuéramos', 'fueran', 'fueras', 'fueron', 'fuese', 'fueseis', 'fuésemos','fuesen', 'fueses', #SER 동사활용형(4)\n",
        "                                 'será', 'serán', 'serás', 'seré', 'seréis', 'seremos', #SER 동사활용형(5)\n",
        "                                 'sería', 'seríais', 'seríamos', 'serían', 'serías', #SER 동사활용형(6)\n",
        "                                 'sido', 'siendo', #SER 동사활용형(7)\n",
        "\n",
        "                                 'estar', 'está', 'estais', 'estáis', 'estamos', 'estan', 'están','estás', 'estoy',  #ESTAR 동사활용형(1)\n",
        "                                 'estaba', 'estabais', 'estábamos', 'estaban', 'estabas', #ESTAR 동사활용형(2)\n",
        "                                 'estada', 'estadas', 'estando', 'estad', #ESTAR 동사활용형(3)\n",
        "                                 'estará', 'estarán', 'estarás', 'estaré', 'estaréis', 'estaremos', #ESTAR 동사활용형(4)\n",
        "                                 'estaría', 'estaríais', 'estaríamos', 'estarían', 'estarías', #ESTAR 동사활용형(5)\n",
        "                                 'esté', 'estéis', 'estemos', 'estén', 'estés', #ESTAR 동사활용형(6) \n",
        "                                 'estuve', 'estuviera', 'estuvierais', 'estuviéramos', 'estuvieran', 'estuvieras', 'estuvieron', #ESTAR 동사활용형(7)\n",
        "                                 'estuviese', 'estuvieseis', 'estuviésemos', 'estuviesen', 'estuvieses', 'estuvimos', 'estuviste', 'estuvisteis', 'estuvo',  #ESTAR 동사활용형(8)\n",
        "\n",
        "                                 'haber', 'ha', 'habéis', 'han', 'has', 'hay', 'he', 'hemos',  #HABER 동사활용형(1)\n",
        "                                 'habia', 'había', 'habíais', 'habíamos', 'habían', 'habías',  #HABER 동사활용형(2)\n",
        "                                 'habida', 'habidas', 'habido', 'habidos', 'habiendo',         #HABER 동사활용형(3)\n",
        "                                 'habrá', 'habrán', 'habrás', 'habré', 'habréis', 'habremos',  #HABER 동사활용형(4)\n",
        "                                 'habría', 'habríais', 'habríamos', 'habrían', 'habrías',      #HABER 동사활용형(5)\n",
        "                                 'haya', 'hayáis', 'hayamos', 'hayan', 'hayas',                #HABER 동사활용형(6)\n",
        "                                 'hube', 'hubimos', 'hubiste', 'hubisteis', 'hubo',            #HABER 동사활용형(7)\n",
        "                                 'hubiera', 'hubierais', 'hubiéramos', 'hubieran', 'hubieras', 'hubieron',  #HABER 동사활용형(8) \n",
        "                                 'hubiese', 'hubieseis', 'hubiésemos', 'hubiesen', 'hubieses', #HABER 동사활용형(9) \n",
        "\n",
        "                                 'ni','no','jamás','nunca','nada','ningún','ninguna','ninguno', #부정어(negativos)\n",
        "                                 'pero','mas', 'aunque','pues','porque','si','sí', #접속사(conjunciones)\n",
        "                                 'algo','algún','alguna','algunas','alguno','algunos', #부정어(indefinidos)\n",
        "                                 'embargo',                                     #기타(etc)                                                                   ))         \n",
        "  \n",
        "                                   \"LBR\",\n",
        "                                   \"ttl\",\n",
        "                                   \"mtl\",\n",
        "                                   \"ftl\",\n",
        "                                   \"=\",\"===============================================\",\n",
        "                                   \"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\"]"
      ],
      "metadata": {
        "id": "fQXEpuoHqKMR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop = set(stopwords.words('spanish'))\n",
        "additional_stopwords=set(black_list)\n",
        "stopwords = stop.union(additional_stopwords)"
      ],
      "metadata": {
        "id": "HvdtnQJeq98_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "!python -m spacy download es_core_news_sm\n",
        "nlp = spacy.load(\"es_core_news_sm\")"
      ],
      "metadata": {
        "id": "HTiSGf6_rEDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatization(texts):\n",
        "    texts_out = [token.text for token in nlp(texts) if token.text not in black_list and len(token.text)>2]\n",
        "    return texts_out"
      ],
      "metadata": {
        "id": "2d9a8dBznPUd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect phrases, based on collected collocation counts. Adjacent words that appear together more frequently than expected are joined together with the _ character.\n",
        "bigram = gensim.models.Phrases(df_news.texto.to_list())"
      ],
      "metadata": {
        "id": "HZXpUnfi6dOq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaner(word):\n",
        "  word = re.sub(r'((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\.([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*', '', word, flags=re.MULTILINE)\n",
        "  word = re.sub(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', \"\", word)\n",
        "  word = re.sub(r'ee.uu', 'eeuu', word)\n",
        "  word = re.sub(r'\\#\\.', '', word)\n",
        "  word = re.sub(r'\\n', '', word)\n",
        "  word = re.sub(r',', '', word)\n",
        "  word = re.sub(r'\\-', ' ', word)\n",
        "  word = re.sub(r'\\.{3}', ' ', word)\n",
        "  word = re.sub(r'a{2,}', 'a', word)\n",
        "  word = re.sub(r'é{2,}', 'é', word)\n",
        "  word = re.sub(r'i{2,}', 'i', word)\n",
        "  word = re.sub(r'ja{2,}', 'ja', word) \n",
        "  word = re.sub(r'á', 'a', word)\n",
        "  word = re.sub(r'é', 'e', word)\n",
        "  word = re.sub(r'í', 'i', word)\n",
        "  word = re.sub(r'ó', 'o', word)\n",
        "  word = re.sub(r'ú', 'u', word)  \n",
        "  word = re.sub('[^a-zA-Z]', ' ', word)\n",
        "  list_word_clean = []\n",
        "  for w1 in word.split(\" \"):\n",
        "    if  w1.lower() not in stopwords:\n",
        "      list_word_clean.append(w1.lower())\n",
        "\n",
        "  bigram_list = bigram[list_word_clean]\n",
        "  # 재조합해서 lemmatize한 list\n",
        "  out_text = lemmatization(\" \".join(bigram_list))\n",
        "  return out_text"
      ],
      "metadata": {
        "id": "n_3wAGvHnTSW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_news['texto'] = df_news['texto'].apply(cleaner)"
      ],
      "metadata": {
        "id": "WzlZ8FlLrcDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### corpus"
      ],
      "metadata": {
        "id": "cIosAjhsrdd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = Dictionary(df_news['texto'].to_list())\n",
        "dictionary.compactify() #??\n",
        "# Filter extremes\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.97, keep_n=None)\n",
        "dictionary.compactify() #??\n",
        "\n",
        "corpus = [dictionary.doc2bow(text) for text in df_news['texto'].to_list()]"
      ],
      "metadata": {
        "id": "MoqHmNhHre5-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tfidf = models.TfidfModel(corpus)\n",
        "# corpus_tfidf = tfidf[corpus]"
      ],
      "metadata": {
        "id": "8iWwG8OnOtwt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate models"
      ],
      "metadata": {
        "id": "7Snf31_FQ6Uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_topics(model, model_type=\"lda\"): # for all models\n",
        "  for topic_idx, topic in enumerate(model.print_topics()):\n",
        "    print (\"Topic %d:\" % (topic_idx))\n",
        "    if model_type== \"hdp\":\n",
        "      print (\" \".join(re.findall( r'\\*(.[^\\*-S]+).?', topic[1])), \"\\n\")\n",
        "    else:\n",
        "      print (\" \".join(re.findall( r'\\\"(.[^\"]+).?', topic[1])), \"\\n\")"
      ],
      "metadata": {
        "id": "mysUf0HEr6-r"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Model#1: Hierarchical Dirichlet process Model**"
      ],
      "metadata": {
        "id": "v_lMuA4XrnXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "time.clock = time.time\n",
        "hdpmodel = HdpModel(corpus=corpus, id2word=dictionary, random_state= 30)"
      ],
      "metadata": {
        "id": "SHG7hYxCr3YD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# evaluate N of topics for LSI, LDA"
      ],
      "metadata": {
        "id": "F1xZnAeARNwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_graph(dictionary, corpus, texts, limit, model): # for LSI, LDA\n",
        "    \"\"\"\n",
        "    Function to display num_topics - LDA graph using c_v coherence\n",
        "    \n",
        "    Parameters:\n",
        "    ----------\n",
        "    dictionary : Gensim dictionary\n",
        "    corpus : Gensim corpus\n",
        "    limit : topic limit\n",
        "    \n",
        "    Returns:\n",
        "    -------\n",
        "    lm_list : List of LDA topic models\n",
        "    c_v : Coherence values corresponding to the LDA model with respective number of topics\n",
        "    \"\"\"\n",
        "    c_v = []\n",
        "    lm_list = []\n",
        "    for num_topics in range(1, limit):\n",
        "        if model == 'lsi':\n",
        "          lm = LsiModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
        "        else:\n",
        "          lm = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
        "        lm_list.append(lm)\n",
        "        cm = CoherenceModel(model=lm, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        c_v.append(cm.get_coherence())\n",
        "        \n",
        "    # Show graph\n",
        "    x = range(1, limit)\n",
        "    plt.plot(x, c_v)\n",
        "    plt.xlabel(\"num_topics\")\n",
        "    plt.ylabel(\"Coherence score\")\n",
        "    plt.legend((\"c_v\"), loc='best')\n",
        "    plt.show()\n",
        "    \n",
        "    return lm_list, c_v"
      ],
      "metadata": {
        "id": "bJZNVNLpr77U"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Model#2: LSI Model**"
      ],
      "metadata": {
        "id": "dSdNRkzwruDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lmlist_lsi, c_v = evaluate_graph(dictionary=dictionary, corpus=corpus, texts=df_news['texto'].to_list(), limit=21, model= \"lsi\")"
      ],
      "metadata": {
        "id": "4ejOEzZdsfvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Model#3: Latent Dirichlet Allocation Model**"
      ],
      "metadata": {
        "id": "xCQopweArvSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lmlist_lda, c_v = evaluate_graph(dictionary=dictionary, corpus=corpus, texts=df_news['texto'].to_list(), limit=21, model= \"lda\")"
      ],
      "metadata": {
        "id": "nu3IgpL6sqjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## summary"
      ],
      "metadata": {
        "id": "Ik-yrJhTsu6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ldamodel = lmlist_lda[5]\n",
        "lsimodel = lmlist_lsi[2]\n",
        "\n",
        "lsitopics = [[word for word, prob in topic] for topicid, topic in lsimodel.show_topics(formatted=False)]\n",
        "hdptopics = [[word for word, prob in topic] for topicid, topic in hdpmodel.show_topics(formatted=False)]\n",
        "ldatopics = [[word for word, prob in topic] for topicid, topic in ldamodel.show_topics(formatted=False)]"
      ],
      "metadata": {
        "id": "H2VUKYGosy72"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hdp_coherence = CoherenceModel(topics=hdptopics[:10], texts=df_news['texto'].to_list(), dictionary=dictionary, window_size=10).get_coherence() \n",
        "lsi_coherence = CoherenceModel(topics=lsitopics[:10], texts=df_news['texto'].to_list(), dictionary=dictionary, window_size=10).get_coherence() \n",
        "lda_coherence = CoherenceModel(topics=ldatopics, texts=df_news['texto'].to_list(), dictionary=dictionary, window_size=10).get_coherence() "
      ],
      "metadata": {
        "id": "YyicIxDts0LC"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "coherences = [hdp_coherence, lsi_coherence, lda_coherence]\n",
        "n = len(coherences)\n",
        "x = ['hdp_coherence', 'lsi_coherence', 'lda_coherence']\n",
        "sns.barplot(x, coherences)"
      ],
      "metadata": {
        "id": "ioSpCKlKs2E4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Classifiying all documents**\n",
        "\n",
        "*  now that we have been select the best model and topics number, is time to assign a topic to each document, means cluster according to the topics\n",
        "\n"
      ],
      "metadata": {
        "id": "hRdD9frctVus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_topics_sentences(ldamodel=0, corpus=corpus, texts=0):\n",
        "    # Init output\n",
        "    sent_topics_df = pd.DataFrame()-n\n",
        "\n",
        "    # Get main topic in each document\n",
        "    for i, row in enumerate(ldamodel[corpus]):\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # => dominant topic\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "\n",
        "    # Add original text to the end of the output\n",
        "    contents = pd.Series(texts)\n",
        "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
        "    return(sent_topics_df)\n",
        "\n",
        "\n",
        "df_topic_sents_keywords = format_topics_sentences(ldamodel, corpus=corpus, texts=df_news['texto'].to_list()) #HY: text -> texto"
      ],
      "metadata": {
        "id": "1GIyFKV4te4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format\n",
        "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
        "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
        "\n",
        "# Show\n",
        "df_dominant_topic.head(500)"
      ],
      "metadata": {
        "id": "dtTKmIW1tgG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We selected the ldamodel with 12 topics and asigned a dominant topic to each document, now let map each topic with a label "
      ],
      "metadata": {
        "id": "hwQ9x2rBthmK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "first let's create the dictionary"
      ],
      "metadata": {
        "id": "-UlgNzJytkn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_dicc = {0:'TEMA1', 1:'TEMA2', 2:'TEMA3', 3: 'TEMA4', 4:'TEMA5', 5:'TEMA6', 6:'TEMA7', \n",
        "              7:'TEMA8', 8:'TEMA9', 9: 'TEMA10', 10:'TEMA11', 11:'TEMA12'}   #HY: 다시 설정 필요!!"
      ],
      "metadata": {
        "id": "66DsHVBOtiQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dominant_topic['Dominant_Topic'] = df_dominant_topic['Dominant_Topic'].astype('int64')"
      ],
      "metadata": {
        "id": "zLtttUbmton-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dominant_topic['Dominant_Topic'] = df_dominant_topic['Dominant_Topic'].map(label_dicc)\n",
        "#df_dominant_topic.head(10)\n",
        "df_dominant_topic"
      ],
      "metadata": {
        "id": "sTZd2n-JtqLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_news['labels'] = df_dominant_topic['Dominant_Topic']"
      ],
      "metadata": {
        "id": "te0icoK0tqlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's examine some text and its topics."
      ],
      "metadata": {
        "id": "Mufi6S8ltsNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_news[ df_news['labels'] == 'TEMA5'].head().texto"
      ],
      "metadata": {
        "id": "rVm-DEq9tv0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's see the distribution of topics.**"
      ],
      "metadata": {
        "id": "05B66e98txav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax = df_dominant_topic['Dominant_Topic'].value_counts().plot(kind='bar')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iLLKhG7wt39F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The topis are almost balanced, so we are good"
      ],
      "metadata": {
        "id": "0cd5OnF1t6IE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "finally that we have our models set up, as well as analyzed, we can go ahead to visualizing them."
      ],
      "metadata": {
        "id": "hAeUQrHXt8ce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "id": "LAYmBzj5uzNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis\n",
        "pyLDAvis.enable_notebook()"
      ],
      "metadata": {
        "id": "G4bc0HDGu1IC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyLDAvis.gensim\n",
        "pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary)"
      ],
      "metadata": {
        "id": "uRubllieu2jZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}